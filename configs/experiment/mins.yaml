# @package _global_

# to execute this experiment run:
# python train.py experiment=mins

defaults:
  - _self_
  - override /data: default
  - override /model: dual_mlp
  - override /task: design_bench
  - override /logger: wandb # set logger here or use command line (e.g. `python train.py logger=tensorboard`)
  - override /trainer: dual_mlp
  - override /paths: default
  - override /hydra: default
  - override /loss: nll
  - override /search: grad

run_name: MINs

train: false 
num_solutions: 128

use_conv: false 

trainer: 
  max_epochs: 100 
  save_best_val_epoch: false

hidden_size: 1024

base_temp: 0.1

gan_data:
  _target_: src.data.datamodule.JAXDataModule
  batch_size: 128
  val_split: 0.2

discriminator:
  _target_: src.model.gan_component.Discriminator
  hidden: ${hidden_size}
  method: wasserstein

discrete_generator: 
  _target_: src.model.gan_component.DiscreteGenerator
  latent_size: 32
  hidden: ${hidden_size}

continuous_generator: 
  _target_: src.model.gan_component.ContinuousGenerator
  latent_size: 32
  hidden: ${hidden_size}

gan_trainer:
  _target_: src.trainer.mins.weight_gan_trainer.WeightGANTrainer
  critic_frequency: 10
  flip_frac: 0.
  fake_pair_frac: 0.
  penalty_weight: 10.
  generator_opt: 
    _target_: optax.adam
    learning_rate: 0.0002
    b1: 0.0
    b2: 0.9
  discriminator_opt:
    _target_: optax.adam
    learning_rate: 0.0002
    b1: 0.0
    b2: 0.9
  max_epochs: 100
  noise_std: 0.0
  keep: 1.0
  start_temp: 5.0
  final_temp: 1.0
  save_prefix: gan
  save_checkpoint_epochs: 10
  checkpoint_dir: ${paths.output_dir}/checkpoints

wandb_api: 9f59486bed008c431a4a5804c35bb3c065d0b658