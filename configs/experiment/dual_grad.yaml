# @package _global_

# to execute this experiment run:
# python train.py experiment=example


defaults:
  - _self_
  - override /data: default
  - override /model: dual_mlp
  - override /task: design_bench
  - override /logger: wandb # set logger here or use command line (e.g. `python train.py logger=tensorboard`)
  - override /trainer: dual_mlp
  - override /paths: default
  - override /hydra: default
  - override /loss: nll
  - override /search: grad

run_name: Gradient-Ascent-Dual-MLP

train: true

trainer: 
  max_epochs: 100 

search:
  learning_rate: 
    discrete: 0.01
    continuous: 0.01
  
  search_steps:
    discrete: 200
    continuous: 200

  scale_lr: true

save_best_val_epoch: false

wandb_api: 9f59486bed008c431a4a5804c35bb3c065d0b658